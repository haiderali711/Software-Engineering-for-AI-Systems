{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyZX8KyTTVFp"
   },
   "source": [
    "# DIT821 Software Engineering for AI systems\n",
    "\n",
    "#### Exam  2020-08-17\n",
    "\n",
    "Reinforcement Learning - Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNin5EQ-TVFq"
   },
   "source": [
    "Enter here your first and last name and your e-mail adress as registered in Canvas.\n",
    "\n",
    "* Name, e-mail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UsINNupTVFq"
   },
   "source": [
    "In this assignement, you will implement the value iteration algorithm on a given gridworld environment (MDP).\n",
    "\n",
    "In order to run this lab you have to install the [gym environment](https://gym.openai.com/docs/) first.\n",
    "In order to install it, simply run in the terminal:\n",
    "```\n",
    "    pip3 install gym\n",
    "```\n",
    "or\n",
    "```\n",
    "    pip install gym\n",
    "```\n",
    "It is assumed that you have installed numpy already, otherwise install it (e.g. as with gym).\n",
    "\n",
    "Please make sure that you have the lib folder in the same directory of this file. \n",
    "\n",
    "Write the code  and comments according to the requirement, and run it.\n",
    "\n",
    "Donwload the file as a notenook file (.ipynb) and submit it to canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peaILrd3TVFr"
   },
   "outputs": [],
   "source": [
    "# Make sure that everything here is imported correctly\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/Users/macmini/anaconda3/lib/python3.7/site-packages')\n",
    "import gym\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the environment\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "\n",
    "```\n",
    "SFFF     \n",
    "FHFH   \n",
    "FFFH     \n",
    "HFFG      \n",
    "```\n",
    "where:\n",
    "```\n",
    "S: starting point, safe\n",
    "F: frozen surface, safe\n",
    "H: hole, fall to your doom\n",
    "G: goal, where the frisbee is located\n",
    "```\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "\n",
    "In the following cell we will create the environment, reset the position of the agent to the starting position and render it. The red square indicates the current position of the player.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_fH2mdsTVFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IxVcyFxVzDn"
   },
   "source": [
    "we can inspect the possible actions to perform in the environment, as well as the possible states of the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNScNBhyVzDn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:  Discrete(4)\n",
      "Observation space:  Discrete(16)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space: \", env.action_space)\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pumeE81OTVFy"
   },
   "source": [
    "The returned objects are of the type Discrete, which describes a discrete space of size n. For example, the action_space for the Frozen Lake environment is a discrete space of 4 values, which means that the possible values for this space are 0 (zero), 1, 2 and 3. The observation_space is a discrete space of 16 values, which goes from 0 to 15.\n",
    "\n",
    "The `sample()` method which returns a random value from the space. With this method, we can easily create a dummy agent that plays the game randomly.\n",
    "\n",
    "\n",
    "To take a step in the environment we use the function `env.step(action)`\n",
    "\n",
    "The environment’s step function returns exactly what we need. In fact, step returns four values. These are:\n",
    "\n",
    "`observation (object)`: an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "\n",
    "`reward (float)`: amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    "\n",
    "`done (boolean)`: whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    "\n",
    "`info (dict)`: diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOFUm3r3VzDq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 1000\n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()\n",
    "env.render()\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(random_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above executes the game for a maximum of 10 iterations using the method sample() from the action_space object to select a random action. Then the env.step() method takes the action as input, executes the action on the environment and returns a tuple of four values:\n",
    "\n",
    "```\n",
    "new_state: the new state of the environment\n",
    "reward: the reward\n",
    "done: a boolean flag indicating if the returned state is a terminal state\n",
    "info: an object with additional information for debugging purposes\n",
    "```\n",
    "\n",
    "Note in the previous output the cases in which the player moves in a different direction than the one chosen by the agent. This behavior is completely normal in the Frozen Lake environment because it simulates a slippery surface. \n",
    "\n",
    "**the transitions from one state to another, for a given action, are probabilistic.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dwehN6ZVzDv"
   },
   "source": [
    "### Q-Learning \n",
    "According to what we have seen in class and what you have done in the labs. Compleate the following code when necessary.\n",
    "\n",
    "You have to implement a Q-learning agent.\n",
    "\n",
    "\n",
    "In order to get the maximum Q-value with respect to all the actions allowed in `state`, use:\n",
    "`np.max(Q[state,:])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SljiDspuVzDy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Episode: 0 Reward: 0 Steps Taken: 7\n",
      "Percent of episodes finished successfully: 0.0\n",
      "Percent of episodes finished successfully (last 100 episodes): 0.0\n",
      "Average number of steps: 7.00\n",
      "Average number of steps (last 100 episodes): 0.07\n",
      "Final Q-Table Values\n",
      "          left          down          right          up\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Set learning parameter\n",
    "\n",
    "# Maximum number of episodes\n",
    "number_episodes = 1\n",
    "\n",
    "# Maximum number of steps in each episode\n",
    "number_iterations = 100\n",
    "\n",
    "# Epsilon, for epsilon-greedy\n",
    "epsilon = 0.9\n",
    "\n",
    "# Learning Rate\n",
    "alpha = 0.81\n",
    "\n",
    "# Discount Factor\n",
    "gamma = 0.96\n",
    "\n",
    "\n",
    "# Initialization of the Q table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation parameters\n",
    "# Steps for each episode\n",
    "# That is: List where each element is the number of steps took by the agent in order to reach a terminal state\n",
    "steps_total = []\n",
    "\n",
    "# Reward got at each episode\n",
    "rewards_total = []\n",
    "\n",
    "\n",
    "\n",
    "# Utility function that returns true with propability epsilon\n",
    "def flipCoin():\n",
    "  return np.random.uniform(0, 1) < epsilon\n",
    "\n",
    "\n",
    "\n",
    "# Get optimal action according the Q-table\n",
    "def get_optimal_action(state):\n",
    "    # Optimal action from the Q-values\n",
    "    action = np.argmax(Q[state,:])\n",
    "    return action\n",
    "\n",
    "    \n",
    "# Choose an action according to an 'epsilon-greedy' strategy\n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(number_episodes):\n",
    "    \n",
    "    # Reset environment and get first new observation\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Reset the steps at each episode\n",
    "    step = 0\n",
    "    \n",
    "    # Reset the reward for each episode\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for iteration in range(number_iterations):\n",
    "        \n",
    "        # *** INSERT CODE HERE ****   \n",
    "        env.render()\n",
    "\n",
    "        action = choose_action(state)  \n",
    "\n",
    "        state2, reward, done, info = env.step(action)  \n",
    "\n",
    "#         learn(state, state2, reward, action)\n",
    "        \n",
    "        predict = Q[state, action]\n",
    "        target = reward + gamma * np.max(Q[state2, :])\n",
    "        Q[state, action] = Q[state, action] + alpha * (target - predict)\n",
    "        \n",
    "        state = state2\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # If a terminal state was reached, i.e. an episode finished, update parameters for evaluation and break the loop\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            if episode % 100 == 0:\n",
    "                print('Episode: {} Reward: {} Steps Taken: {}'.format(episode, episode_reward, step))\n",
    "            break\n",
    "    \n",
    "\n",
    "    \n",
    "# EVALUATION\n",
    "        \n",
    "print(\"Percent of episodes finished successfully: {0}\".format(sum(rewards_total)/number_episodes))\n",
    "print(\"Percent of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/number_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Final Q-Table Values\")\n",
    "print (\"          left          down          right          up\")\n",
    "print (Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can run the game according the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 10\n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()\n",
    "env.render()\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    state = env.reset()\n",
    "    optimal_action = get_optimal_action(state)\n",
    "    print(optimal_action)\n",
    "    new_state, reward, done, info = env.step(optimal_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fudirgqKTVGQ"
   },
   "source": [
    "## Submit the solution\n",
    "\n",
    "When you completed the excercise, download (form File menu) this file as a jupyter Notebook file (.ipynb) and uplaod this file in the CANVAS \n",
    "\n",
    "By writing down my name I declare that we have done the assignement myself:\n",
    "\n",
    "* First Name  Last Name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exam-NeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

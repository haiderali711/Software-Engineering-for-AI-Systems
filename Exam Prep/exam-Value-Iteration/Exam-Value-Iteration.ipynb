{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyZX8KyTTVFp"
   },
   "source": [
    "# DIT821 Software Engineering for AI systems\n",
    "\n",
    "#### Exam  2020-08-17\n",
    "\n",
    "Reinforcement Learning - Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNin5EQ-TVFq"
   },
   "source": [
    "Enter here your first and last name and your e-mail adress as registered in Canvas.\n",
    "\n",
    "    * Name Shab Pompeiano, e-mail: guspomsh@student.gu.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UsINNupTVFq"
   },
   "source": [
    "In this assignement, you will implement the value iteration algorithm on a given gridworld environment (MDP).\n",
    "\n",
    "In order to run this lab you have to install the [gym environment](https://gym.openai.com/docs/) first.\n",
    "In order to install it, simply run in the terminal:\n",
    "```\n",
    "    pip3 install gym\n",
    "```\n",
    "or\n",
    "```\n",
    "    pip install gym\n",
    "```\n",
    "It is assumed that you have installed numpy already, otherwise install it (e.g. as with gym).\n",
    "\n",
    "Please make sure that you have the lib folder in the same directory of this file. \n",
    "\n",
    "Write the code  and comments according to the requirement, and run it.\n",
    "\n",
    "Donwload the file as a notenook file (.ipynb) and submit it to canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peaILrd3TVFr"
   },
   "outputs": [],
   "source": [
    "# Make sure that everything here is imported correctly\n",
    "import numpy as np\n",
    "import sys\n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the gridworld\n",
    "The next command generated a 4x4 grid and saves the MDP in the variable `env`.\n",
    "\n",
    "The gridwold looks like this:\n",
    "\n",
    "    T  o  o  o\n",
    "    o  x  o  o\n",
    "    o  o  o  o\n",
    "    o  o  o  T\n",
    "\n",
    "Where x is the position of the agent and T are the terminal states.\n",
    "\n",
    "The agent, at each state, can take 4 actions: `UP=0, RIGHT=1, DOWN=2, LEFT=3`\n",
    "\n",
    "Both states and action are represented with integer values.\n",
    "\n",
    "Actions going off the edge leave you in your current state.\n",
    "\n",
    "The agent receives a reward of -1 at each step until you reach a terminal state. If the state is terminal every action will give the agent a reward of 1.\n",
    "\n",
    "***`env.P`*** represents the transition probabilities of the environment.\n",
    "\n",
    "***`env.P[s][a]`***: given a state `s` and an action `a`, it returns a list of transition tuples `(prob, next_state, reward, done)` where `prob` is the probability associated to the next state (`next_state`), `reward` is the reward associated to the transition and `done` is a boolean indicating if `next_state` is terminal.\n",
    "\n",
    "***`env.nS`*** is a number of states in the environment (16 in this example). \n",
    "\n",
    "***`env.nA`*** is a number of actions in the environment (4 in this example).\n",
    "\n",
    "For more information, feel free to consult the `gridworld.py` file inside the `lib/envs/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_fH2mdsTVFv"
   },
   "outputs": [],
   "source": [
    "# Load the 4x4 gridworld environment inside the variable 'env'\n",
    "\"\"\" env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "    env.nS is a number of states in the environment. \n",
    "    env.nA is a number of actions in the environment.\"\"\"\n",
    "env = GridworldEnv()\n",
    "\n",
    "# Fixing the discount factor\n",
    "discount_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IxVcyFxVzDn"
   },
   "source": [
    "####  Getting the q-values (values for each action from a given state): \n",
    "\n",
    "The following function will help you in the development of your algorithm.\n",
    "\n",
    "It takes a state `state` and the numpy Vector `V` of size `env.nS` containing the current values for each state.\n",
    "\n",
    "It return the values of `state` after step look-ahead for ***all*** the actions. Since we have four actions avilable at each state it returns 4 values of `state`, one for each action.\n",
    "\n",
    "It is the core of the value function algorithm. Check out the slides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNScNBhyVzDn"
   },
   "outputs": [],
   "source": [
    "def get_values_one_step(state, V):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the value for all action in a given state.\n",
    "\n",
    "    Args:\n",
    "        state: The state to consider (int)\n",
    "        env:   OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "        V:     The vector of currect values to use as an estimator, Vector of length env.nS\n",
    "        discount_factor: discount factor\n",
    "\n",
    "    Returns:\n",
    "        A vector of length env.nA containing the expected value of each action.\n",
    "    \"\"\"\n",
    "    # Vector to return     \n",
    "    A = np.zeros(env.nA)\n",
    "    \n",
    "    # *** INSERT CODE HERE ***\n",
    "    # TIP: Use env.P[state][action] to get the list of (probability, next_state, reward, done).\n",
    "    \n",
    "    for i in range (env.nA):\n",
    "        for prob, next_state, reward, done in env.P[state][i]:\n",
    "            A[i] = A[i] + prob * (discount_factor * V[next_state] + reward)\n",
    "            \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pumeE81OTVFy"
   },
   "source": [
    "#### In the next cell, you should  complete the value iteration function using the `get_q_values_for_state` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOFUm3r3VzDq"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.        \n",
    "    Returns:\n",
    "        V: numpy Vector containing the optimal value for each state of env.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize to zero the numpy Vector containing env.nS values, one for each state of the MDP.\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        \n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            # keep old state value in memory for calculating the delta            \n",
    "            old_s = V[s]\n",
    "            \n",
    "            # Update the value function, i.e. the vector V\n",
    "            # *** INSERT CODE HERE ***\n",
    "            #  TIP: Do a one-step lookahead from state s to find the best action\n",
    "\n",
    "            A = get_values_one_step(s, V)\n",
    "            \n",
    "            best_action = np.argmax(A) \n",
    "            \n",
    "            q = 0\n",
    "            for prob, next_state, reward, done in env.P[s][best_action]:\n",
    "                q = q + prob * (discount_factor * V[next_state] + reward)\n",
    "            \n",
    "            V[s] = q\n",
    "            \n",
    "            \n",
    "            # Calculate delta across all states seen so far (for stopping condition)\n",
    "            delta = max(delta, np.abs(V[s] - old_s))\n",
    "            \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Policy (BONUS)\n",
    "Once we have all the optimal values for each state we can compute the optimal Policy\n",
    "It is not mandatory to complete the following part of the lab as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOrTLyTgTVFz"
   },
   "outputs": [],
   "source": [
    "def compute_optimal_policy(V):\n",
    "    \"\"\"\n",
    "    Computes the optival policy\n",
    "    \n",
    "    Args:\n",
    "        V: numpy Vector containing the optimal values for each state in env\n",
    "        \n",
    "    Returns:\n",
    "        policy: some data structure returning the best action for each state\n",
    "    \"\"\"\n",
    "    # ***INSERT CODE HERE***\n",
    "    # TIP: Create a deterministic policy using the optimal value function\n",
    "    \n",
    "    policy = []\n",
    "    \n",
    "    for s in V:\n",
    "        max_q = env.P[s][0]\n",
    "        for i in env.nA:\n",
    "            if max_q <env.P[s][i]:\n",
    "                max_q = env.P[s][i]\n",
    "                best_action = i\n",
    "        policy.push(best_action)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dwehN6ZVzDv"
   },
   "source": [
    "### Check if your solution is correct\n",
    "Run the following code to check if the value functions that your functions generates are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SljiDspuVzDy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONGRATS! THE SOLUTION YOU HAVE PROVIDED IS CORRECT!\n"
     ]
    }
   ],
   "source": [
    "v = value_iteration(env)\n",
    "\n",
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "try:\n",
    "    np.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "    print(\"CONGRATS! THE SOLUTION YOU HAVE PROVIDED IS CORRECT!\")\n",
    "except AssertionError as e :\n",
    "    print(\"SOLUTION NOT CORRECT\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fudirgqKTVGQ"
   },
   "source": [
    "## Submit the solution\n",
    "\n",
    "When you completed the excercise, download (form File menu) this file as a jupyter Notebook file (.ipynb) and uplaod this file in the CANVAS \n",
    "\n",
    "By writing down my name I declare that we have done the assignement myself:\n",
    "\n",
    "* First Name: Shab  Last Name: Pompeiano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exam-NeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
